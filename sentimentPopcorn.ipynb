{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读入数据，并且用beautsoup去除杂七杂八的网页标签，review返回一个list，list反映content的各个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeltrain = pd.read_csv(\"labeledTrainData.tsv\",header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeltrain = pd.read_csv(\"unlabeledTrainData.tsv\",header=0, delimiter=\"\\t\", quoting=3)\n",
    "testdata = pd.read_csv(\"testData.tsv\",header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getwords(review,remove_stopwords=False):\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        document = open('/Users/houxiaohui/anaconda/lib/python2.7/site-packages/nltk_data/corpora/stopwords/english')\n",
    "        stopwords = []\n",
    "        for word in document:\n",
    "            stopwords.append(word)\n",
    "        stops = set(stopwords)\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = labeltrain.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#因为要使用TF-IDF，因此要有一个包含了所有单词的全文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/houxiaohui/anaconda/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "train_words = []\n",
    "for i in range(len(labeltrain)):\n",
    "    review = labeltrain['review'][i]\n",
    "    wordlist = getwords(review)\n",
    "    train_words.append(\" \".join(wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_words = []\n",
    "for i in range(len(testdata)):\n",
    "    review = testdata['review'][i]\n",
    "    wordlist = getwords(review)\n",
    "    test_words.append(\" \".join(wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer as TFIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfv = TFIV(min_df=3,  max_features=None, strip_accents='unicode', analyzer='word',\\\n",
    "           token_pattern=r'\\w{1,}', ngram_range=(1, 2), use_idf=1,smooth_idf=1,\\\n",
    "           sublinear_tf=1, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_all = train_words+test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_all = tfv.fit_transform(x_all)\n",
    "x = x_all[:len(train_words)]\n",
    "x_test = x_all[len(train_words):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "nb = MNB()\n",
    "nb.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5折交叉验证得分:  0.948345504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "print \"5折交叉验证得分: \",np.mean(cross_val_score(nb,x,y,cv=5,scoring='roc_auc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict = nb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"id\":testdata['id'],\"sentiment\":predict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#利用TF-IDF和多项贝叶斯在test集上得到的结果是0.85724。很不解，朴素贝叶斯的overfit很小，但五折交叉验证明显优于测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用word2vec进行模型预测 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在TF-IDF中，为了更好地计算每个词的重要性，把停用词去掉，但是word2vec中最好不要去掉。因为word2vec的词语向量h会会根据停用词算出。TF-IDF中喂给算法的是单词，但在word2vec中最好的喂给算法的是一个句子。但是怎么样才能中断句子呢？科学家们认为，即使用标点符号，大小写等也是不够准确的，python有一个nltk包可以帮助我们做到区分句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#安装nltk包，并且下载相关语料包\n",
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#加载标点符号分词器\n",
    "tokenizer = nltk.data.load('/Users/houxiaohui/anaconda/lib/python2.7/site-packages/tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义一个函数划分句子\n",
    "def review_to_sentences(review, tokenizer,remove_stopwords=False):\n",
    "    #用NLTK分词器把段落划分成为句子\n",
    "    review = review.decode(\"utf-8\")    #把review解码，否则报错\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    #sentences[i]是一个包含了该句话所有单词组成的list\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(getwords( raw_sentence,remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "把labeltrain的句子划分出来\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/houxiaohui/anaconda/lib/python2.7/site-packages/bs4/__init__.py:198: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/Users/houxiaohui/anaconda/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "把unlabeltrain的句子划分出来\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/houxiaohui/anaconda/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/houxiaohui/anaconda/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/houxiaohui/anaconda/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/houxiaohui/anaconda/lib/python2.7/site-packages/bs4/__init__.py:198: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/Users/houxiaohui/anaconda/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/houxiaohui/anaconda/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "print \"把labeltrain的句子划分出来\"\n",
    "for review in labeltrain['review']:\n",
    "    sentences+= review_to_sentences(review,tokenizer)\n",
    "print \"把unlabeltrain的句子划分出来\"\n",
    "for review in unlabeltrain['review']:\n",
    "    sentences+=review_to_sentences(review,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'maybe', u'i', u'just', u'want', u'to', u'get', u'a', u'certain', u'insight', u'into', u'this', u'guy', u'who', u'i', u'thought', u'was', u'really', u'cool', u'in', u'the', u'eighties', u'just', u'to', u'maybe', u'make', u'up', u'my', u'mind', u'whether', u'he', u'is', u'guilty', u'or', u'innocent']\n"
     ]
    }
   ],
   "source": [
    "print sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the built-in logging module and configure it so that Word2Vec \n",
    "#creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model\n"
     ]
    }
   ],
   "source": [
    "num_features = 300\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3\n",
    "from gensim.models import word2vec\n",
    "print \"training model\"\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers,size=num_features,min_count=min_word_count,\\\n",
    "                         window =context, sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#如果不打算更改参数，init_sims可以令到模型更加有效\n",
    "model.init_sims(replace=True)\n",
    "#保存模型\n",
    "model_name = \"300features_40windows_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16490, 300)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#把保存的model加载出来\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"300features_40windows_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[-0.06180373 -0.03055314 -0.02788739 -0.00303928  0.05845078  0.02961411\n",
      "  0.05386321  0.01109022  0.05256153 -0.0449115   0.02647414 -0.05939076\n",
      "  0.05391535  0.05881985 -0.07142239 -0.08370717  0.0069117  -0.06156367\n",
      " -0.02996042 -0.02917108 -0.05412992  0.00463001  0.05070795  0.04969992\n",
      " -0.02298744  0.04569796 -0.03031595 -0.04911005 -0.02743537  0.07245769\n",
      " -0.00487353 -0.01499925 -0.0151846   0.02670506 -0.01087801  0.06588218\n",
      "  0.00762568 -0.01244507  0.07263737  0.00819124  0.00708139 -0.00480718\n",
      " -0.05014724 -0.0069901   0.00170102  0.0041009  -0.01330999 -0.04403951\n",
      " -0.01806104  0.07172135  0.02140717  0.02385642  0.04330316  0.01740825\n",
      "  0.01324386  0.07636381  0.00933965  0.0473394  -0.11823727  0.02022422\n",
      " -0.05764703  0.01486576  0.06008713  0.00676664 -0.05963673  0.13316357\n",
      "  0.019418    0.01395135  0.10422439 -0.05284138 -0.05169625  0.01932217\n",
      "  0.02650642 -0.00288717 -0.01929533 -0.04086848 -0.03387094  0.04883172\n",
      " -0.03066172  0.06306476  0.02690616  0.02531354 -0.02972518 -0.13778333\n",
      " -0.05149676  0.05046259 -0.02304406 -0.07606448  0.10841538 -0.0197446\n",
      " -0.10194106  0.0190563  -0.04865722  0.03666922 -0.0606392  -0.00482855\n",
      "  0.02781078 -0.08294812  0.00612506  0.04400071 -0.04638592 -0.05084838\n",
      " -0.01644487 -0.12121135  0.05126842 -0.07814528  0.02690852  0.04704968\n",
      " -0.01408321 -0.05060897  0.05957375  0.02557756  0.13246141 -0.02264337\n",
      "  0.03731392 -0.01274165  0.0773737   0.03135191  0.10775499  0.10784682\n",
      " -0.0251552   0.00212042 -0.06087171  0.05856616  0.02581641 -0.0699223\n",
      " -0.04915842 -0.031783    0.09388897 -0.08907373 -0.03517028  0.04018539\n",
      " -0.05186386 -0.00576534  0.0008856   0.00325637 -0.04870493 -0.05612949\n",
      " -0.06789903 -0.01321625  0.01363876  0.00526933  0.02061844 -0.01824184\n",
      "  0.10787617  0.01974935  0.03650636  0.06788897 -0.01475046 -0.00156207\n",
      "  0.00325741 -0.00399916  0.02487342 -0.02199714 -0.05283765  0.0319464\n",
      " -0.11083597  0.03335205  0.01868506  0.02472123 -0.14904514  0.06959432\n",
      " -0.00966812 -0.07574466  0.01920062  0.00697869 -0.07115431 -0.00678842\n",
      "  0.08777456  0.12896286 -0.08950039 -0.15202707  0.01265506 -0.12884825\n",
      "  0.1280812  -0.00867368 -0.07842692 -0.09505848 -0.01116284 -0.07155107\n",
      "  0.01383694  0.08384699 -0.10190263 -0.00392841 -0.00297837 -0.02503533\n",
      "  0.02370019 -0.08058197  0.02922229 -0.04630927  0.02104352 -0.04894935\n",
      "  0.00326188  0.05768817  0.03892397  0.08047204 -0.05944632  0.0325316\n",
      " -0.01083004  0.0115817   0.03165441  0.07487994 -0.02877281  0.00633848\n",
      "  0.02721304 -0.0403351  -0.07461401 -0.04416452 -0.00097146  0.09901249\n",
      " -0.00253382  0.05049176  0.02210064  0.0680978   0.12689149 -0.01855649\n",
      "  0.05843979  0.01375236  0.09957723  0.04389399  0.09190293 -0.02088717\n",
      " -0.05491019  0.00203879 -0.01607464 -0.00666108  0.0257586  -0.07611056\n",
      "  0.03667893 -0.0016688  -0.04186317 -0.08090296  0.01790336 -0.12422229\n",
      "  0.02610609 -0.0183766   0.01516562 -0.08748685  0.06955118  0.02589136\n",
      "  0.04349745  0.04647666 -0.01170536  0.03198835  0.10258437  0.05678932\n",
      "  0.05995404  0.07367473  0.09955543  0.04123604  0.05492217 -0.06072693\n",
      " -0.09196123  0.10154213  0.04177253  0.04951245 -0.02826471 -0.03604344\n",
      "  0.15655226  0.00035826 -0.0363954   0.05937751  0.01739688  0.05481123\n",
      " -0.01166699 -0.00755732  0.10924341  0.04473987  0.00666339  0.00502951\n",
      "  0.00085368 -0.06203975 -0.06260986  0.09569049 -0.04678449 -0.07485842\n",
      "  0.01203846 -0.05874245  0.04115397  0.00722081 -0.07874727  0.08510511\n",
      "  0.1123427   0.01874532 -0.06127739  0.05736314 -0.04582794 -0.06320956\n",
      "  0.03008085 -0.02977003  0.08055408  0.00593114  0.0472064  -0.02116339\n",
      " -0.09668767  0.00017825 -0.17414665  0.08469169  0.10186158  0.0702939 ]\n"
     ]
    }
   ],
   "source": [
    "#flower 300维向量表示\n",
    "print model[\"flower\"].shape\n",
    "print model['flower']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在单词的维度统一了，但是不同的review的向量还没有得出，以下代码要实现的是把单词每个维度取平均数，赋给每个review，弄成一个矩阵\n",
    "此时，移除停用词，否则会加入太多的噪声"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    #该方程算出所有单词的300维中的每一维的平均向量\n",
    "    # Function to average all of the word vectors in a given paragraph\n",
    "    featureVec = np.zeros((num_features),dtype=float)\n",
    "    nwords = 0\n",
    "    \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords+=1\n",
    "            featureVec =np.add(featureVec,model[word])\n",
    "    \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    #Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    #计算每一个review的平均向量，返回一个2D的矩阵\n",
    "    \n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=float)\n",
    "    \n",
    "    for review in reviews:\n",
    "        if counter%1000 == 0:\n",
    "            print \"Review %d of %d\"%(counter, len(reviews))\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter+1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为了防止噪声干扰，移除停用词\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "print '''为了防止噪声干扰，移除停用词'''\n",
    "clean_train_reviews = []\n",
    "for review in labeltrain['review']:\n",
    "    clean_train_reviews.append(getwords(review, remove_stopwords=True))\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews,model,num_features)\n",
    "\n",
    "clean_test_reviews = []\n",
    "for review in testdata['review']:\n",
    "    clean_test_reviews.append(getwords(review, remove_stopwords=True))\n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在得到了每个review在300维的向量，放到随机森林上进行建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用随机森林算法拟合训练集\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "forest = forest.fit(trainDataVecs, y)\n",
    "y_hat = forest.predict(testDataVecs)\n",
    "\n",
    "output = pd.DataFrame( data={\"id\":testdata[\"id\"], \"sentiment\":y_hat} )\n",
    "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word2vec加上随机森林在测试集上的LB是0.82940"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既然Word2Vec能把单词变成向量，那么可以通过向量进行聚类，看看哪些单词是一堆的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "利用kmeans聚类单词所用的时间：  1456.46670794\n"
     ]
    }
   ],
   "source": [
    "#每5个单词一类\n",
    "start = time.time()\n",
    "wordVectors = model.syn0\n",
    "num_clusters = wordVectors.shape[0]/5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "idx = km.fit_predict(wordVectors)\n",
    "end = time.time()\n",
    "print \"利用kmeans聚类单词所用的时间： 分钟\", int(end-start)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'iceberg']\n",
      "[u'hayward']\n",
      "[u'layers', u'revelations', u'explanations']\n",
      "[u'roaming', u'roam']\n",
      "[u'blade', u'runner', u'blazing', u'cape', u'claw', u'cobra', u'raider']\n",
      "[u'hysteria', u'oppression', u'liberation', u'persecution', u'probable', u'secular']\n",
      "[u'whit', u'gibbs', u'weber']\n",
      "[u'swords', u'armor', u'elephants', u'barf']\n",
      "[u'whipped', u'nailed', u'pinned', u'battered']\n",
      "[u'pompous', u'smug', u'smarmy', u'dishonest', u'humorless', u'overplayed', u'unintelligent']\n"
     ]
    }
   ],
   "source": [
    "#现在单词向量被分成了很多类，但是人是看不懂向量的，要把向量转成单词\n",
    "allwords = model.index2word\n",
    "for cluster in range(10):\n",
    "    words = []\n",
    "    for i in range(len(wordVectors)):\n",
    "        if idx[i] == cluster:\n",
    "            words.append(allwords[i])\n",
    "    print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用每个向量去做随机森林太耗费时间，既然已经聚类出来了，可以用每个单词的聚类作为特征，喂给随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_centroid_map = dict(zip(model.index2word,idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(review,):\n",
    "    #返回一个数列，第i行代表第i类有多少个单词\n",
    "    bag_of_centroids = np.zeros(num_clusters,dtype=float)\n",
    "    for word in review:\n",
    "        if word_centroid_map.has_key(word):\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index]+=1\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_centroids = np.zeros((len(labeltrain['review']),num_clusters),dtype=float)\n",
    "for i in range(len(clean_train_reviews)):\n",
    "    review = clean_train_reviews[i]\n",
    "    train_centroids[i] = create_bag_of_centroids(review)\n",
    "test_centroids = np.zeros((len(testdata['review']), num_clusters),dtype=float)\n",
    "for i in range(len(clean_test_reviews)):\n",
    "    review = clean_test_reviews[i]\n",
    "    test_centroids[i] = create_bag_of_centroids(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "print \"Fitting a random forest to labeled training data...\"\n",
    "forest = forest.fit(train_centroids,y)\n",
    "result = forest.predict(test_centroids)\n",
    "# Write the test results \n",
    "#output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "#output.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 改进算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改进TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#上面的word2vec返回的单词go和went是不一样的，nltk中提供了提取单词词根的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "english_stemmer=nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getwords(review,remove_stopwords=False):\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        document = open('/Users/houxiaohui/anaconda/lib/python2.7/site-packages/nltk_data/corpora/stopwords/english')\n",
    "        stopwords = []\n",
    "        for word in document:\n",
    "            stopwords.append(word)\n",
    "        stops = set(stopwords)\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #提取特征词\n",
    "    stemmerWords = []\n",
    "    for word in words:\n",
    "        stemmerWords.append(english_stemmer.stem(word))\n",
    "    return stemmerWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为了防止噪声干扰，移除停用词\n"
     ]
    }
   ],
   "source": [
    "print '''为了防止噪声干扰，移除停用词'''\n",
    "clean_train_reviews = []\n",
    "for review in labeltrain['review']:\n",
    "    clean_train_reviews.append(getwords(review, remove_stopwords=True))\n",
    "    \n",
    "clean_test_reviews = []\n",
    "for review in testdata['review']:\n",
    "    clean_test_reviews.append(getwords(review, remove_stopwords=True))\n",
    "\n",
    "clean_unlabeltrain_reviews = []\n",
    "for review in unlabeltrain['review']:\n",
    "    clean_unlabeltrain_reviews.append(getwords(review, remove_stopwords=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_train_reviews = []\n",
    "for reviewlist in clean_train_reviews:\n",
    "    cleaned_train_reviews.append(\" \".join(reviewlist))\n",
    "    \n",
    "cleaned_unlabeltrain_reviews = []\n",
    "for reviewlist in clean_unlabeltrain_reviews:\n",
    "    cleaned_unlabeltrain_reviews.append(\" \".join(reviewlist))\n",
    "    \n",
    "cleaned_test_reviews = []\n",
    "for reviewlist in clean_test_reviews:\n",
    "    cleaned_test_reviews.append(\" \".join(reviewlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizing words\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print \"vectorizing words\"\n",
    "vectorizer = TfidfVectorizer( min_df=2, ngram_range = ( 1, 4 ),\n",
    "                              sublinear_tf = True )\n",
    "vectorizer = vectorizer.fit(cleaned_train_reviews + cleaned_unlabeltrain_reviews)\n",
    "train_data_features = vectorizer.transform(cleaned_train_reviews)\n",
    "test_data_features = vectorizer.transform(cleaned_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection.univariate_selection import SelectKBest, chi2, f_classif\n",
    "fselect = SelectKBest(chi2 , k=70000)\n",
    "train_data_features = fselect.fit_transform(train_data_features, y)\n",
    "test_data_features = fselect.transform(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "model1 = MultinomialNB(alpha=0.0005)\n",
    "model1.fit( train_data_features, y)\n",
    "\n",
    "model2 = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\n",
    "model2.fit( train_data_features,y )\n",
    "\n",
    "p1 = model1.predict( test_data_features )\n",
    "p2 = model2.predict( test_data_features )\n",
    "\n",
    "print \"Writing results...\"\n",
    "\n",
    "output = pd.DataFrame( data = { \"id\": testdata[\"id\"], \"sentiment\": p1 } )\n",
    "output.to_csv(\"TFIDFmultiNB.csv\", index = False, quoting = 3 )\n",
    "\n",
    "output = pd.DataFrame( data = { \"id\": testdata[\"id\"], \"sentiment\": p2 } )\n",
    "output.to_csv(\"TFIDFSGBClassifier.csv\", index = False, quoting = 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进word2vec的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "word2vec_AverageVectors在做review向量化的时候，把每个单词的每个维度的向量相加\n",
    "这种方式太简单粗暴\n",
    "之前的Word2vecmodel，最小计数单词是40个，这个要求太严苛了\n",
    "num_features = 300\n",
    "min_word_count = 5\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3\n",
    "from gensim.models import word2vec\n",
    "print \"training model\"\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers,size=num_features,min_count=min_word_count,\\\n",
    "                         window =context, sample=downsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加入doc2vec算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word2vec在训练在像微博这样的140个词语以内的短文是有效的，但是当训练长文的时候就需要doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCleanWords(data,remove_stopwords):\n",
    "    clean_reviews = []\n",
    "    for review in data['review']:\n",
    "        clean_reviews.append(getwords(review,remove_stopwords))\n",
    "    \n",
    "    #doc2vec训练句子的时候要有一个label\n",
    "    labelized = []\n",
    "    for i, id_label in enumerate(data['id']):\n",
    "        labelized.append(LabeledSentence(clean_reviews[i], [id_label]))\n",
    "    return labelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelizedLableTrain = getCleanWords(labeltrain,remove_stopwords=True)\n",
    "labelizedUnlableTrain = getCleanWords(unlabeltrain,remove_stopwords=True)\n",
    "labelizedTestData = getCleanWords(testdata,remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 2000\n",
    "min_word_count = 10\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainging data with Doc2Vec\n"
     ]
    }
   ],
   "source": [
    "print \"trainging data with Doc2Vec\"\n",
    "model_dm = Doc2Vec(min_count=min_word_count, window=context, size=num_features, \\\n",
    "                           sample=downsampling, workers=num_workers)\n",
    "model_dbow = Doc2Vec(min_count=min_word_count, window=context, size=num_features, \n",
    "                             sample=downsampling, workers=num_workers, dm=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_reviews = np.concatenate((labelizedLableTrain + labelizedUnlableTrain + labelizedTestData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-fcdd4dbb7b28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_dm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_reviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_dbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_reviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/houxiaohui/anaconda/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, keep_raw_vocab, trim_rule, progress_per)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \"\"\"\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# initial survey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# trim by min_count & precalculate downsampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# build tables & arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/houxiaohui/anaconda/lib/python2.7/site-packages/gensim/models/doc2vec.pyc\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, documents, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdocument_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m                     logger.warn(\"Each 'words' should be a list of words (usually unicode strings).\"\n\u001b[1;32m    659\u001b[0m                                 \"First 'words' here is instead plain %s.\" % type(document.words))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "model_dm.build_vocab(all_reviews)\n",
    "model_dbow.build_vocab(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "for epoch in range(10):\n",
    "    perm = np.random.permutation(all_reviews.shape[0])\n",
    "    model_dm.train(all_reviews[perm])\n",
    "    model_dbow.train(all_reviews[perm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeatureVecs(reviews, model, num_features):\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    counter = -1\n",
    "    \n",
    "    for review in reviews:\n",
    "        counter = counter + 1\n",
    "        try:\n",
    "            reviewFeatureVecs[counter] = np.array(model[review.labels[0]]).reshape((1, num_features))\n",
    "        except:\n",
    "            continue\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "getFeatureVecs(labelizedLableTrain, model_dm, n_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结：\n",
    "采用了三种方法对单词的向量进行处理，分别是TF-IDF, Word2Vec， Doc2Vec\n",
    "TF-IDF利用了单词出现的频率\n",
    "Word2Vec是和Doc2Vec是利用神经网络，具体原理不懂\n",
    "### Word2Vec\n",
    "1. 利用tokenize把review切成句子\n",
    "2. 每句单词进行清洗，包括去除奇奇怪怪的字符，只保留英文单词，停用词有选择去除或者不去除\n",
    "3. 切成句子之后的单词要分开，喂给word2vec的是一个list，list中的每一项也是一个list，里面都是清洗好的单词\n",
    "4. word2vec.Word2Vec（sentences） 把每个单词转化为向量就能够当做特征进行分类、降维、聚类\n",
    "\n",
    "### Doc2Vec\n",
    "1. 同样clean word,但是要给每个sentences一个label\n",
    "2. 有dm和dow两种模型\n",
    "3. 要循环几次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
